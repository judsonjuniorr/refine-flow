# LangChain Integration

## VisÃ£o Geral

O RefineFlow agora usa LangChain para otimizar prompts, gerenciar tokens automaticamente e suportar mÃºltiplos modelos OpenAI com suas configuraÃ§Ãµes especÃ­ficas.

## Arquitetura

### Componentes Principais

1. **models.py** - ConfiguraÃ§Ã£o de limites de tokens por modelo
2. **langchain_prompts.py** - Templates de prompts estruturados com parsers
3. **client_langchain.py** - Cliente OpenAI usando LangChain ChatOpenAI
4. **processor_langchain.py** - LÃ³gica de processamento usando chains

## Modelos Suportados

O sistema agora suporta automaticamente:

### GPT-4 Series
- `gpt-4` (8K input / 8K output)
- `gpt-4-32k` (32K input / 32K output)
- `gpt-4-turbo`, `gpt-4-turbo-2024-04-09`, `gpt-4-turbo-preview` (128K input / 4K output)
- `gpt-4-1106-preview`, `gpt-4-0125-preview` (128K input / 4K output)

### GPT-4o Series  
- `gpt-4o`, `gpt-4o-2024-05-13`, `gpt-4o-2024-08-06` (128K input / 16K output)
- `gpt-4o-mini`, `gpt-4o-mini-2024-07-18` (128K input / 16K output)

### GPT-3.5 Series
- `gpt-3.5-turbo`, `gpt-3.5-turbo-16k` (16K input / 4K output)
- `gpt-3.5-turbo-1106`, `gpt-3.5-turbo-0125` (16K input / 4K output)

### O1 Series (Reasoning Models)
- `o1` (200K input / 100K output)
- `o1-preview` (128K input / 32K output)
- `o1-mini` (128K input / 65K output)

### Outros
- `gpt-5-mini` (128K input / 4K output)

## OtimizaÃ§Ã£o AutomÃ¡tica de Tokens

O sistema calcula automaticamente o `max_tokens` baseado no tipo de tarefa:

| Tipo de Tarefa | % do Max Output | Uso |
|----------------|-----------------|-----|
| **extraction** | 30% | ExtraÃ§Ã£o de entidades e atualizaÃ§Ã£o de estado |
| **chat** | 50% | Respostas a perguntas |
| **jira** | 60% | GeraÃ§Ã£o de export Jira |
| **canvas** | 70% | GeraÃ§Ã£o de Business Case Canvas |

### Exemplo

Para `gpt-4o` (max output: 16K tokens):
- extraction: 4,800 tokens
- chat: 8,000 tokens
- jira: 9,600 tokens
- canvas: 11,200 tokens

## Modelos de RaciocÃ­nio (Reasoning Models)

Os modelos O1 series (`o1`, `o1-preview`, `o1-mini`) tÃªm restriÃ§Ãµes especiais:

- âœ… **Suportam:** `max_completion_tokens`
- âŒ **NÃ£o suportam:** parÃ¢metro `temperature` (sempre usa valor padrÃ£o)

O sistema detecta automaticamente esses modelos e remove o parÃ¢metro `temperature`.

## Prompts Estruturados

Todos os prompts agora usam `ChatPromptTemplate` com mensagens separadas:

```python
EXTRACTION_TEMPLATE = ChatPromptTemplate.from_messages([
    ("system", """VocÃª Ã© um assistente especializado..."""),
    ("human", """Entrada do usuÃ¡rio:
{entry_text}

Contexto atual:
{current_state}""")
])
```

### BenefÃ­cios

1. **SeparaÃ§Ã£o clara** entre instruÃ§Ãµes do sistema e input do usuÃ¡rio
2. **ReutilizaÃ§Ã£o** de prompts em diferentes contextos
3. **Parsing automÃ¡tico** de respostas com validaÃ§Ã£o

## Output Parsing

### JsonOutputParser

Usado para `process_entry()` - valida e converte JSON para `ActivityState`:

```python
chain = prompt | llm | JsonOutputParser()
result = chain.invoke(inputs)  # Retorna dict validado
```

### StrOutputParser

Usado para `answer_question()`, `generate_jira_export()`, `generate_canvas()`:

```python
chain = prompt | llm | StrOutputParser()
result = chain.invoke(inputs)  # Retorna string
```

## Logging Detalhado

O sistema agora registra:

- âœ… Modelo usado e seus limites de tokens
- âœ… Max tokens calculado para a tarefa
- âœ… DetecÃ§Ã£o de reasoning model
- âœ… Metadata da resposta (tokens usados, finish_reason)
- âœ… Erros de parsing com mensagens claras

## MigraÃ§Ã£o do CÃ³digo Antigo

### Antes (Direct OpenAI API)

```python
from refineflow.llm.processor import LLMProcessor

processor = LLMProcessor()
state = processor.process_entry(activity, entry, state)
```

### Agora (LangChain)

```python
from refineflow.llm.processor_langchain import LLMProcessor

processor = LLMProcessor()
state = processor.process_entry(activity, entry, state)
```

A interface pÃºblica permanece a mesma! Apenas mude o import.

## Arquivos Criados/Modificados

### Novos Arquivos

- `src/refineflow/llm/models.py` (106 linhas)
- `src/refineflow/llm/langchain_prompts.py` (166 linhas)
- `src/refineflow/llm/client_langchain.py` (135 linhas)
- `src/refineflow/llm/processor_langchain.py` (210 linhas)

### Arquivos Modificados

- `pyproject.toml` - Adicionadas dependÃªncias LangChain
- `src/refineflow/cli/flows.py` - Import atualizado para `processor_langchain`
- `src/refineflow/core/exporters.py` - Import atualizado, novo mÃ©todo `generate_canvas()`
- `tests/test_config.py` - Teste ajustado para aceitar mÃºltiplos modelos

## DependÃªncias Adicionadas

```toml
langchain = ">=0.1.0"
langchain-openai = ">=0.0.5"
langchain-core = ">=0.1.0"
tiktoken = ">=0.5.0"
```

## InstalaÃ§Ã£o

```bash
# Criar ambiente virtual
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows

# Instalar dependÃªncias
pip install -e ".[dev]"
```

## Testes

Todos os 56 testes continuam passando:

```bash
pytest tests/ -v
```

## PrÃ³ximos Passos

1. âœ… **ImplementaÃ§Ã£o base completa**
2. âœ… **Testes passando**
3. ğŸ”„ **Testar com API real do OpenAI**
4. ğŸ”„ **Validar JSON parsing com entradas reais**
5. ğŸ”„ **Otimizar prompts baseado em feedback**
6. ğŸ“‹ **Adicionar suporte para novos modelos conforme disponibilidade**

## SoluÃ§Ã£o de Problemas

### Erro: "max_tokens not supported"

**Causa:** Modelo O1 series nÃ£o suporta `max_tokens`  
**SoluÃ§Ã£o:** AutomÃ¡tica - o sistema usa `max_completion_tokens`

### Erro: "temperature does not support X with this model"

**Causa:** Modelo O1 series nÃ£o suporta parÃ¢metro `temperature`  
**SoluÃ§Ã£o:** AutomÃ¡tica - o sistema detecta e remove o parÃ¢metro

### Erro: "Output parsing failed"

**Causa:** LLM retornou JSON invÃ¡lido ou formato incorreto  
**SoluÃ§Ã£o:** Verifique os logs detalhados, ajuste o prompt se necessÃ¡rio

### Respostas vazias

**Causa:** PossÃ­vel problema com parÃ¢metros do modelo ou prompt  
**SoluÃ§Ã£o:** 
1. Verifique logs para ver JSON completo da resposta
2. Confirme que o modelo estÃ¡ configurado em `models.py`
3. Teste com modelo conhecido (ex: `gpt-4-turbo`)

## ReferÃªncias

- [LangChain Documentation](https://python.langchain.com/)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference)
- [Model Token Limits](https://platform.openai.com/docs/models)
