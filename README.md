# RefineFlow

Agente de refinamento de atividades com IA e CLI interativa para gerenciamento de tarefas tÃ©cnicas.

> **Nota**: A interface do usuÃ¡rio estÃ¡ completamente em portuguÃªs brasileiro (pt-BR).

## âœ¨ Novidades na v0.2.0

ğŸš€ **IntegraÃ§Ã£o LangChain Completa**
- âœ… Prompts otimizados com templates estruturados
- âœ… Gerenciamento automÃ¡tico de tokens por modelo (20+ modelos OpenAI)
- âœ… Suporte para reasoning models (O1 series) com detecÃ§Ã£o automÃ¡tica
- âœ… Output parsing com validaÃ§Ã£o (JSON + String)
- âœ… Logging detalhado de tokens e metadata

ğŸ“– [Leia mais sobre a implementaÃ§Ã£o LangChain](LANGCHAIN.md)

## Funcionalidades

- ğŸ¯ **Gerenciamento de Contexto**: Crie e mantenha contextos detalhados para cada atividade tÃ©cnica
- ğŸ¤– **AnÃ¡lise com IA**: ExtraÃ§Ã£o automÃ¡tica de itens de aÃ§Ã£o, questÃµes, decisÃµes e lacunas
- ğŸ“Š **Business Case Canvas**: Gere documentaÃ§Ã£o abrangente de business case
- ğŸ« **ExportaÃ§Ã£o Jira**: Exporte atividades como tarefas pai com subtarefas backend/frontend
- ğŸ’¬ **Chat Interativo**: Perguntas e respostas com contexto e citaÃ§Ãµes
- ğŸ“ **Armazenamento Markdown**: Todos os dados armazenados em arquivos Markdown legÃ­veis
- ğŸ” **Busca Inteligente**: Ãndice SQLite para pesquisa rÃ¡pida de atividades

## InstalaÃ§Ã£o

### Requisitos

- Python 3.12 ou superior
- Chave de API OpenAI
- (Opcional) Ollama para embeddings

### ConfiguraÃ§Ã£o

#### OpÃ§Ã£o 1: Setup AutomÃ¡tico (Recomendado)

```bash
git clone <repository-url>
cd refinement-agent
./setup.sh
```

O script irÃ¡:
- Criar ambiente virtual
- Instalar todas as dependÃªncias
- Configurar .env
- Opcionalmente iniciar Ollama com Docker

#### OpÃ§Ã£o 2: Setup Manual

1. Clone o repositÃ³rio:
```bash
git clone <repository-url>
cd refinement-agent
```

2. Crie um ambiente virtual:
```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

3. Instale o pacote:
```bash
pip install -e ".[dev]"
```

**Ou use o Makefile:**
```bash
make install
```

4. Configure as variÃ¡veis de ambiente:
```bash
cp .env.example .env
# Edite .env e adicione sua chave de API OpenAI
```

> **Dica**: Use `make help` para ver todos os comandos disponÃ­veis.

## ConfiguraÃ§Ã£o

### ConfiguraÃ§Ã£o ObrigatÃ³ria

Defina sua chave de API OpenAI no `.env`:
```env
OPENAI_API_KEY=sua-chave-api-aqui
OPENAI_MODEL=gpt-5-mini
```

### Opcional: Ollama para Embeddings

Se vocÃª quiser usar embeddings para busca semÃ¢ntica, hÃ¡ duas opÃ§Ãµes:

#### OpÃ§Ã£o 1: Usando Docker Compose (Recomendado)

1. Inicie o Ollama com Docker Compose:
```bash
docker-compose up -d
```

O container irÃ¡:
- Subir o servidor Ollama na porta 11434
- Baixar automaticamente o modelo `snowflake-arctic-embed`
- Manter os dados persistentes no volume Docker

2. Configure no `.env`:
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=snowflake-arctic-embed
ENABLE_EMBEDDINGS=true
```

3. Verificar status:
```bash
docker-compose logs -f ollama
```

4. Parar o serviÃ§o:
```bash
docker-compose down
```

#### OpÃ§Ã£o 2: InstalaÃ§Ã£o Local

1. Instale o [Ollama](https://ollama.ai/)
2. Baixe o modelo de embedding:
```bash
ollama pull snowflake-arctic-embed
```
3. Configure no `.env`:
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=snowflake-arctic-embed
ENABLE_EMBEDDINGS=true
```

## Uso

### Iniciar a AplicaÃ§Ã£o

```bash
refineflow
```

Ou usando o mÃ³dulo Python:
```bash
python -m refineflow
```

### Fluxos Principais

#### 1. Criar uma Nova Atividade

1. Selecione "ğŸ“ Criar Nova Atividade" no menu principal
2. Responda as perguntas de inicializaÃ§Ã£o:
   - TÃ­tulo da Atividade
   - Breve DescriÃ§Ã£o
   - DeclaraÃ§Ã£o do Problema
   - Stakeholders (separados por vÃ­rgula)
   - RestriÃ§Ãµes/Cronograma
   - Sistema/Produto Afetado

O sistema cria uma estrutura de pastas com templates Markdown.

#### 2. Trabalhar em uma Atividade

1. Selecione "ğŸ”„ Selecionar Atividade em Andamento"
2. Escolha dentre as atividades disponÃ­veis
3. Visualize o painel de status com resumo e aÃ§Ãµes abertas
4. OpÃ§Ãµes:
   - Adicionar informaÃ§Ã£o (notas, decisÃµes, transcriÃ§Ãµes, etc.)
   - Ver questÃµes abertas (categorizadas por Frontend, Backend, Arquitetura, Produto, UX/UI)
   - Conversar com contexto
   - Gerar Business Case Canvas
   - Exportar para Jira
   - Finalizar atividade

#### 3. Adicionar InformaÃ§Ã£o

Selecione o tipo de entrada:
- **Nota**: ObservaÃ§Ã£o ou informaÃ§Ã£o geral
- **Resposta**: Resposta a uma pergunta anterior
- **TranscriÃ§Ã£o**: TranscriÃ§Ã£o de reuniÃ£o ou conversa
- **DecisÃ£o**: DecisÃ£o documentada
- **Requisito**: Requisito funcional ou nÃ£o-funcional
- **Risco**: Risco identificado e mitigaÃ§Ã£o
- **MÃ©trica**: MÃ©trica de sucesso ou KPI
- **Custo**: Estimativa de custo ou item de orÃ§amento
- **DependÃªncia**: DependÃªncia interna ou externa

> **Nota**: Perguntas nÃ£o sÃ£o adicionadas manualmente. Elas sÃ£o **extraÃ­das automaticamente pelo LLM** a partir do conteÃºdo das entradas e categorizadas por domÃ­nio tÃ©cnico (Frontend, Backend, Arquitetura, Produto, UX/UI, Geral).

Escolha o mÃ©todo de entrada:
- **MÃºltiplas linhas (terminal)**: Digite diretamente no terminal (ESC + Enter ou Ctrl+D para finalizar)
- **Editor do Sistema**: Abre o editor padrÃ£o do sistema ($EDITOR)

#### 4. Modo ConversaÃ§Ã£o

FaÃ§a perguntas sobre o contexto da atividade. A IA irÃ¡:
- Usar todo o contexto disponÃ­vel de logs e estado
- Citar fontes (nomes de arquivos e timestamps)
- Fornecer insights relevantes

Digite 'sair' para retornar ao menu.

#### 5. Gerar Business Case Canvas

Cria um documento abrangente de business case cobrindo:
- Problema e SoluÃ§Ã£o
- Recursos e DependÃªncias
- BenefÃ­cios e ROI
- Escopo e Cronograma
- Riscos e MitigaÃ§Ãµes
- Stakeholders
- AnÃ¡lise de Complexidade
- Plano de ComunicaÃ§Ã£o
- Custos e MÃ©tricas

O canvas destaca informaÃ§Ãµes faltantes e sugere perguntas para completÃ¡-lo.

#### 6. Exportar para Jira

Gera:
- Tarefa pai com contexto completo
- Subtarefa de backend
- Subtarefa de frontend

Formatos de exportaÃ§Ã£o:
- **Markdown**: Formatado para copiar e colar
- **JSON**: Dados estruturados
- **CSV**: CompatÃ­vel com planilhas

#### 7. Finalizar Atividade

- Marca a atividade como completa
- Previne modificaÃ§Ãµes futuras
- Permite consulta e exportaÃ§Ã£o

## Estrutura do Projeto

```
data/
â””â”€â”€ activities/
    â””â”€â”€ <slug-da-atividade>/
        â”œâ”€â”€ activity.md       # VisÃ£o geral e metadados
        â”œâ”€â”€ log.md           # Entradas cronolÃ³gicas
        â”œâ”€â”€ canvas.md        # Business Case Canvas
        â”œâ”€â”€ jira_export.md   # SaÃ­da da exportaÃ§Ã£o Jira
        â”œâ”€â”€ state.json       # Estado estruturado
        â””â”€â”€ chat.md          # HistÃ³rico de conversas
```

## Desenvolvimento

### Comandos RÃ¡pidos com Makefile

```bash
make help          # Lista todos os comandos disponÃ­veis
make install       # Instala o projeto
make test          # Executa testes
make test-cov      # Testes com cobertura
make lint          # Verifica cÃ³digo
make format        # Formata cÃ³digo
make run           # Inicia RefineFlow
make docker-up     # Inicia Ollama
make docker-logs   # Ver logs do Ollama
make docker-down   # Para Ollama
make setup         # Configura tudo (install + docker-up)
```

### Executar Testes

```bash
pytest
# ou
make test
```

### VerificaÃ§Ã£o de Tipos

```bash
mypy src/refineflow
# ou
make type-check
```

### Linting

```bash
ruff check src/refineflow
ruff format src/refineflow
# ou
make format
```

## Arquitetura

- **Armazenamento**: Arquivos Markdown + Ã­ndice SQLite
- **IA**: OpenAI API com **LangChain** para geraÃ§Ã£o e anÃ¡lise de texto
- **Prompts**: Templates estruturados com system/human messages em pt-BR
- **Token Management**: OtimizaÃ§Ã£o automÃ¡tica baseada em modelo e tipo de tarefa
- **Output Parsing**: ValidaÃ§Ã£o automÃ¡tica com JsonOutputParser e StrOutputParser
- **Embeddings**: IntegraÃ§Ã£o opcional com Ollama para busca semÃ¢ntica (via Docker)
- **Interface**: Rich panels e Questionary para menus interativos
- **ConfiguraÃ§Ã£o**: Pydantic settings com variÃ¡veis de ambiente

## Modelos Suportados

RefineFlow suporta automaticamente 20+ modelos OpenAI com otimizaÃ§Ã£o de tokens:

- **GPT-4 Series**: gpt-4, gpt-4-32k, gpt-4-turbo, gpt-4-turbo-preview
- **GPT-4o Series**: gpt-4o, gpt-4o-mini (128K input / 16K output)
- **GPT-3.5 Series**: gpt-3.5-turbo, gpt-3.5-turbo-16k
- **O1 Series** (Reasoning): o1, o1-preview, o1-mini
  - âš ï¸ DetecÃ§Ã£o automÃ¡tica - remove parÃ¢metro `temperature`

Configure no `.env`:
```env
OPENAI_MODEL=gpt-4-turbo  # ou gpt-4o, o1-mini, etc
```

ğŸ“– [Ver lista completa de modelos e limites](LANGCHAIN.md#modelos-suportados)

## Arquivos de ConfiguraÃ§Ã£o

- `docker-compose.yml` - ConfiguraÃ§Ã£o do Ollama com download automÃ¡tico do modelo
- `Makefile` - Comandos Ãºteis para desenvolvimento
- `.env.example` - Template de variÃ¡veis de ambiente
- `pyproject.toml` - ConfiguraÃ§Ã£o do projeto Python
- `DOCKER.md` - DocumentaÃ§Ã£o detalhada do Docker

## LicenÃ§a

MIT
